{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the CrystFEL Processing and Refinement Workflow\n",
    "\n",
    "This notebook provides a comprehensive pipeline for processing crystallography data using CrystFEL tools and custom scripts. The workflow encompasses the following key stages:\n",
    "\n",
    "1. **Indexing on a Circular Grid:**  \n",
    "   Use the `gandalf_iterator` to perform iterative peak finding, indexing, and integration over a radial grid of beam center shifts.\n",
    "\n",
    "2. **Visualization of Indexing Performance:**  \n",
    "   Generate 3D histograms and 2D heatmaps to assess how different beam center adjustments affect indexing success.\n",
    "\n",
    "3. **Evaluation of Indexing Metrics:**  \n",
    "   Process stream files to compute key quality metrics (weighted RMSD, fraction of outliers, deviation measures, etc.) that help quantify indexing accuracy.\n",
    "\n",
    "4. **Interactive Metrics Analysis & CSV-to-Stream Conversion:**  \n",
    "   Use interactive widgets to:\n",
    "   - Filter individual metrics with separate thresholds.\n",
    "   - Create a combined quality metric from weighted inputs.\n",
    "   - Visualize the filtered results.\n",
    "   - Convert the filtered metrics CSV into a stream file for downstream processing.\n",
    "\n",
    "5. **Interactive Merging and Format Conversion:**  \n",
    "   Merge the best indexing results and convert them into crystallographically useful formats:\n",
    "   - **SHELX Conversion:** Transform the merged results into a Shelx-compatible .hkl file.\n",
    "   - **MTZ Conversion:** Convert the .hkl file to the .mtz format for further analysis.\n",
    "\n",
    "6. **Refinement Using REFMAC5:**  \n",
    "   Refine the merged data with REFMAC5 and parse the resulting log file to extract and plot Rf_used values versus resolution, providing insight into the refinement quality.\n",
    "\n",
    "> **Pre-requisites:**  \n",
    "> Ensure that all required processing steps, tools (CrystFEL, REFMAC5, etc.), and Python packages (ipywidgets, matplotlib, etc.) are installed and properly configured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ==============================================\n",
    "# Run Indexamajig Iterations on a Circular Grid\n",
    "### Options for Peak Finding, Indexing, and Integration\n",
    "\n",
    "This section executes the `gandalf_iterator` function to perform:\n",
    "- Peak finding using options tailored for CXI data.\n",
    "- Indexing with XGandalf, including setting tolerances and sampling parameters.\n",
    "- Integration using a rings method.\n",
    "\n",
    "Define your grid parameters (maximum radius and step size) to iterate over beam center shifts on a circular grid.\n",
    "# ==============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gandalf_interations.gandalf_radial_iterator import gandalf_iterator\n",
    "\n",
    "# Hard-coded parameters (adjust these paths and values as needed)\n",
    "geom_file = \"path/to/your_file.geom\"       # Path to your .geom file\n",
    "cell_file = \"path/to/your_file.cell\"        # Path to your .cell file\n",
    "input_folder = \"path/to/your/input_folder\"  # Path to your input folder\n",
    "\n",
    "output_base = \"Xtal\"\n",
    "num_threads = 24\n",
    "max_radius = 1.8\n",
    "step = 0.5\n",
    "\n",
    "# Choose your peakfinder method: 'cxi', 'peakfinder9', or 'peakfinder8'\n",
    "peakfinder = 'cxi'\n",
    "\n",
    "# Define default peakfinder options.\n",
    "default_peakfinder_options = {\n",
    "    'cxi': \"--peaks=cxi\",\n",
    "    'peakfinder9': \"\"\"--peaks=peakfinder9\n",
    "--min-snr=1\n",
    "--min-snr-peak-pix=6\n",
    "--min-snr-biggest-pix=1\n",
    "--min-sig=9 \n",
    "--min-peak-over-neighbour=5\n",
    "--local-bg-radius=5\"\"\",\n",
    "    'peakfinder8': \"\"\"--peaks=peakfinder8\n",
    "--threshold=45\n",
    "--min-snr=3\n",
    "--min-pix-count=3\n",
    "--max-pix-count=500\n",
    "--local-bg-radius=9\n",
    "--min-res=30\n",
    "--max-res=500\"\"\"\n",
    "}\n",
    "\n",
    "# Other extra flags (as a multiline string)\n",
    "other_flags_str = \"\"\"--no-revalidate\n",
    "--no-half-pixel-shift\n",
    "--no-refine\n",
    "--no-non-hits-in-stream\"\"\"\n",
    "\n",
    "# Advanced indexing parameters\n",
    "min_peaks = 15\n",
    "tolerance = \"10,10,10,5\"\n",
    "xgandalf_sampling_pitch = 5\n",
    "xgandalf_grad_desc_iterations = 1\n",
    "xgandalf_tolerance = 0.02\n",
    "int_radius = \"2,5,10\"\n",
    "\n",
    "# Build flags\n",
    "other_flags = [line.strip() for line in other_flags_str.splitlines() if line.strip()]\n",
    "peakfinder_flags = [line.strip() for line in default_peakfinder_options[peakfinder].splitlines() if line.strip()]\n",
    "\n",
    "advanced_flags = [\n",
    "    f\"--min-peaks={min_peaks}\",\n",
    "    f\"--tolerance={tolerance}\",\n",
    "    f\"--xgandalf-sampling-pitch={xgandalf_sampling_pitch}\",\n",
    "    f\"--xgandalf-grad-desc-iterations={xgandalf_grad_desc_iterations}\",\n",
    "    f\"--xgandalf-tolerance={xgandalf_tolerance}\",\n",
    "    f\"--int-radius={int_radius}\"\n",
    "]\n",
    "\n",
    "\"\"\"Examples of extra flags(see crystfel documentation https://www.desy.de/~twhite/crystfel/manual-indexamajig.html):\n",
    "    \n",
    "    Peakfinding\n",
    "    \"--peaks=cxi\",\n",
    "    \"--peak-radius=inner,middle,outer\",\n",
    "    \"--min-peaks=n\",\n",
    "    \"--median-filter=n\",\n",
    "    \"--filter-noise\",\n",
    "    \"--no-revalidate\",\n",
    "    \"--no-half-pixel-shift\",\n",
    "\n",
    "    \"--peaks=peakfinder9\",\n",
    "    \"--min-snr=1\",\n",
    "    \"--min-snr-peak-pix=6\",\n",
    "    \"--min-snr-biggest-pix=1\",\n",
    "    \"--min-sig=9\",\n",
    "    \"--min-peak-over-neighbour=5\",\n",
    "    \"--local-bg-radius=5\",\n",
    "\n",
    "    \"--peaks=peakfinder8\",\n",
    "    \"--threshold=45\",\n",
    "    \"--min-snr=3\",\n",
    "    \"--min-pix-count=3\",\n",
    "    \"--max-pix-count=500\",\n",
    "    \"--local-bg-radius=9\",\n",
    "    \"--min-res=30\",\n",
    "    \"--max-res=500\",\n",
    "    \n",
    "    Indexing\n",
    "    \"--indexing=xgandalf\",\n",
    "\n",
    "    \"--tolerance=tol\"\n",
    "    \"--no-check-cell\",\n",
    "    \"--no-check-peaks\",\n",
    "    \"--multi\",\n",
    "    \"--no-retry\",\n",
    "    \"--no-refine\",\n",
    "\n",
    "    \"--xgandalf-sampling-pitch=n\"\n",
    "    \"--xgandalf-grad-desc-iterations=n\"\n",
    "    \"--xgandalf-tolerance=n\"\n",
    "    \"--xgandalf-no-deviation-from-provided-cell\"\n",
    "    \"--xgandalf-max-lattice-vector-length=n\"\n",
    "    \"--xgandalf-min-lattice-vector-length=n\"\n",
    "    \"--xgandalf-max-peaks=n\"\n",
    "\n",
    "    Integration\n",
    "    \"--fix-profile-radius=n\",\n",
    "    \"--integration=rings\",\n",
    "    \"--int-radius=4,5,10\",\n",
    "    \"--push-res=n\",\n",
    "    \"--overpredict\",\n",
    "\n",
    "    Output\n",
    "    \"--no-non-hits-in-stream\",\n",
    "    \"--no-peaks-in-stream\",\n",
    "    \"--no-refls-in-stream\",\n",
    "\"\"\"\n",
    "\n",
    "# Fixed indexing flags (unchanged)\n",
    "indexing_flags = [\n",
    "    \"--indexing=xgandalf\",\n",
    "    \"--integration=rings\",\n",
    "]\n",
    "\n",
    "# Combine all flags\n",
    "flags_list = advanced_flags + other_flags + peakfinder_flags + indexing_flags\n",
    "\n",
    "# Display the parameters for verification.\n",
    "print(\"Running gandalf_iterator with the following parameters:\")\n",
    "print(\"Geom File:\", geom_file)\n",
    "print(\"Cell File:\", cell_file)\n",
    "print(\"Input Folder:\", input_folder)\n",
    "print(\"Output Base:\", output_base)\n",
    "print(\"Threads:\", num_threads)\n",
    "print(\"Max Radius:\", max_radius)\n",
    "print(\"Step:\", step)\n",
    "print(\"\\nCombined Flags:\", flags_list)\n",
    "\n",
    "# Run the indexing workflow\n",
    "try:\n",
    "    gandalf_iterator(\n",
    "        geom_file,\n",
    "        cell_file,\n",
    "        input_folder,\n",
    "        output_base,\n",
    "        num_threads,\n",
    "        max_radius=max_radius,\n",
    "        step=step,\n",
    "        extra_flags=flags_list\n",
    "    )\n",
    "    print(\"Indexing completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error during indexing:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ==============================================\n",
    "# Visualize Indexing Results: 3D Histogram & 2D Heatmap\n",
    "\n",
    "After running the iterations, this section generates visualizations:\n",
    "- **3D Histogram:** Provides an overview of the indexing rate across the grid.\n",
    "- **2D Heatmap:** Offers a more detailed view of the beam center optimization.\n",
    "\n",
    "Make sure that the output folder path reflects the folder where the iterative results are saved.\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.indexing_3d_histogram import plot3d_indexing_rate\n",
    "from visualization.indexing_center import indexing_heatmap\n",
    "\n",
    "# Hard-coded output folder path (adjust this path as needed)\n",
    "output_folder = \"path/to/your/output_folder\"\n",
    "\n",
    "print(\"Generating visualizations for output folder:\", output_folder)\n",
    "try:\n",
    "    # Call the visualization functions.\n",
    "    plot3d_indexing_rate(output_folder)\n",
    "    indexing_heatmap(output_folder)\n",
    "    print(\"Visualization completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error during visualization:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ==============================================\n",
    "# Process Indexing Metrics Across All Stream Files\n",
    "\n",
    "In this section, the notebook processes all stream file outputs from Indexamajig by:\n",
    "- Reading each stream file and computing key indexing quality metrics.\n",
    "- Evaluating metrics such as weighted RMSD, fraction of outliers, length and angle deviations, peak ratio, and percentage indexed.\n",
    "\n",
    "These metrics will be used later to select the best results for further processing.\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calc_metrics.process_indexing_metrics import process_indexing_metrics\n",
    "\n",
    "# Hard-coded parameter values (adjust these as needed)\n",
    "stream_folder = \"path/to/your/stream_folder\"  # Path to your stream file folder\n",
    "wrmsd_tolerance = 2.0      # WRMSD tolerance (default: 2.0)\n",
    "indexing_tolerance = 4.0   # Indexing tolerance (default: 4.0)\n",
    "\n",
    "print(\"Processing metrics for folder:\", stream_folder)\n",
    "print(\"WRMSD Tolerance:\", wrmsd_tolerance)\n",
    "print(\"Indexing Tolerance:\", indexing_tolerance)\n",
    "\n",
    "try:\n",
    "    process_indexing_metrics(stream_folder, wrmsd_tolerance=wrmsd_tolerance, indexing_tolerance=indexing_tolerance)\n",
    "    print(\"Metrics processed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error processing metrics:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Interactive Metrics Analysis and CSV-to-Stream Conversion\n",
    "\n",
    "This interactive section enables you to:\n",
    "- **Filter Metrics:** Use individual sliders to apply separate thresholds to each quality metric.\n",
    "- **Create a Combined Metric:** Input weights for each metric to generate a composite quality score.\n",
    "- **Visualize the Distribution:** Display histograms of both separate and combined metrics.\n",
    "- **Convert CSV to Stream:** Once the best rows are filtered, convert the CSV file into a stream file for merging.\n",
    "\n",
    "Use the interactive widgets to adjust thresholds and weights, and then trigger the filtering and conversion processes.\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Filter Separate Metrics Section -----\n",
    "import matplotlib.pyplot as plt\n",
    "from filter_and_combine.interactive_iqm import read_metric_csv, get_metric_ranges, filter_rows\n",
    "\n",
    "# ----- Parameter Setup & CSV Loading -----\n",
    "CSV_PATH = \"path/to/your/metrics.csv\"  # Adjust this path as needed\n",
    "\n",
    "print(\"Loading CSV file:\", CSV_PATH)\n",
    "grouped_data = read_metric_csv(CSV_PATH, group_by_event=True)\n",
    "all_rows = [row for rows in grouped_data.values() for row in rows]\n",
    "print(f\"Loaded {len(all_rows)} rows from CSV.\")\n",
    "\n",
    "# Define metrics to be analyzed.\n",
    "metrics_in_order = [\n",
    "    'weighted_rmsd',\n",
    "    'fraction_outliers',\n",
    "    'length_deviation',\n",
    "    'angle_deviation',\n",
    "    'peak_ratio',\n",
    "    'percentage_unindexed'\n",
    "]\n",
    "\n",
    "# ----- Compute Default Ranges & Set Thresholds -----\n",
    "# Compute ranges for each metric from the loaded data.\n",
    "ranges_dict = get_metric_ranges(all_rows, metrics=metrics_in_order)\n",
    "print(\"\\nMetric ranges (min, max):\")\n",
    "for m in metrics_in_order:\n",
    "    print(f\"  {m}: {ranges_dict[m]}\")\n",
    "\n",
    "# Set thresholds for each metric.\n",
    "# By default, thresholds are set to the maximum value from the data.\n",
    "# To adjust a threshold, simply modify the corresponding value in the THRESHOLDS dictionary.\n",
    "THRESHOLDS = {metric: ranges_dict[metric][1] for metric in metrics_in_order}\n",
    "# Example adjustment:\n",
    "# THRESHOLDS['weighted_rmsd'] = 10.0\n",
    "\n",
    "print(\"\\nUsing thresholds:\")\n",
    "for m in metrics_in_order:\n",
    "    print(f\"  {m}: {THRESHOLDS[m]}\")\n",
    "\n",
    "# ----- Filter Data & Plot Histograms -----\n",
    "filtered_separate = filter_rows(all_rows, THRESHOLDS)\n",
    "print(f\"\\nFiltering: {len(all_rows)} total rows -> {len(filtered_separate)} pass thresholds.\")\n",
    "\n",
    "if filtered_separate:\n",
    "    # Plot histograms for each metric.\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    for i, metric in enumerate(metrics_in_order):\n",
    "        values = [r[metric] for r in filtered_separate if metric in r]\n",
    "        axes[i].hist(values, bins=20)\n",
    "        axes[i].set_title(f\"Histogram of {metric}\")\n",
    "        axes[i].set_xlabel(metric)\n",
    "        axes[i].set_ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No rows passed the thresholds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Combine Metrics & Filter Section -----\n",
    "import matplotlib.pyplot as plt\n",
    "from filter_and_combine.interactive_iqm import create_combined_metric, select_best_results_by_event, write_filtered_csv\n",
    "\n",
    "print(\"\\n--- Combined Metric Creation & Filtering ---\")\n",
    "# Define weights for each metric (adjust as needed; default is all zeros here)\n",
    "weights = {metric: 0.0 for metric in metrics_in_order}\n",
    "# Optionally, set some non-zero weights, for example:\n",
    "# weights = {'weighted_rmsd': 0.5, 'fraction_outliers': 0.2, 'length_deviation': 0.1, 'angle_deviation': 0.1, 'peak_ratio': 0.05, 'percentage_unindexed': 0.05}\n",
    "\n",
    "# Create the combined metric in the data rows\n",
    "create_combined_metric(\n",
    "    rows=all_rows,\n",
    "    metrics_to_combine=metrics_in_order,\n",
    "    weights=[weights[m] for m in metrics_in_order],\n",
    "    new_metric_name=\"combined_metric\"\n",
    ")\n",
    "\n",
    "# Determine range of the combined metric\n",
    "combined_vals = [r[\"combined_metric\"] for r in all_rows if \"combined_metric\" in r]\n",
    "if combined_vals:\n",
    "    cmin, cmax = min(combined_vals), max(combined_vals)\n",
    "    # Set threshold to the max value by default (adjust as needed)\n",
    "    combined_threshold = cmax\n",
    "    print(f\"Combined metric created successfully!\")\n",
    "    print(f\"  * Min value: {cmin:.3f}\")\n",
    "    print(f\"  * Max value: {cmax:.3f}\")\n",
    "else:\n",
    "    print(\"Failed to create combined metric. Check your weights.\")\n",
    "\n",
    "# Filter rows by the combined metric threshold\n",
    "filtered_combined = [r for r in all_rows if \"combined_metric\" in r and r[\"combined_metric\"] <= combined_threshold]\n",
    "print(f\"Filtering rows by combined_metric ≤ {combined_threshold:.3f}\")\n",
    "if not filtered_combined:\n",
    "    print(\"No rows passed the combined metric threshold.\")\n",
    "else:\n",
    "    # Group the filtered rows by event number\n",
    "    grouped_filtered = {}\n",
    "    for r in filtered_combined:\n",
    "        event = r.get(\"event_number\")\n",
    "        if event not in grouped_filtered:\n",
    "            grouped_filtered[event] = []\n",
    "        grouped_filtered[event].append(r)\n",
    "    \n",
    "    best_filtered = select_best_results_by_event(grouped_filtered, sort_metric=\"combined_metric\")\n",
    "    print(f\"{len(filtered_combined)} rows passed threshold, {len(best_filtered)} best rows selected per event.\")\n",
    "    \n",
    "    # Write the best filtered rows to a CSV file\n",
    "    write_filtered_csv(best_filtered, FILTERED_CSV_PATH)\n",
    "    print(f\"Wrote {len(best_filtered)} best-filtered rows to {FILTERED_CSV_PATH}\")\n",
    "    \n",
    "    # Plot histogram for the combined metric from the best rows\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    values = [r[\"combined_metric\"] for r in best_filtered]\n",
    "    plt.hist(values, bins=20)\n",
    "    plt.title(\"Histogram of Best Rows (combined_metric)\")\n",
    "    plt.xlabel(\"combined_metric\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Write Filter & Combine Results CSV to STREAM -----\n",
    "import os\n",
    "import time\n",
    "from filter_and_combine.csv_to_stream import write_stream_from_filtered_csv\n",
    "from filter_and_combine.interactive_iqm import read_metric_csv\n",
    "\n",
    "print(\"\\n--- Converting to Stream ---\")\n",
    "# Create output directory for the stream file (subfolder 'filtered_metrics' in the CSV directory)\n",
    "output_dir = os.path.join(os.path.dirname(CSV_PATH), 'filtered_metrics')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the output stream file path\n",
    "OUTPUT_STREAM_PATH = os.path.join(output_dir, 'filtered_metrics.stream')\n",
    "\n",
    "print(\"Starting conversion to stream file...\")\n",
    "time.sleep(0.2)\n",
    "print(\"  * Step 1/5: Reading filtered CSV file...\")\n",
    "filtered_grouped_data = read_metric_csv(FILTERED_CSV_PATH, group_by_event=True)\n",
    "time.sleep(0.2)\n",
    "\n",
    "print(\"  * Step 2/5: Checking for combined metric and selecting best rows (if applicable)...\")\n",
    "time.sleep(0.2)\n",
    "\n",
    "print(\"  * Step 3/5: (Best rows selection already performed in previous cell)\")\n",
    "time.sleep(0.2)\n",
    "\n",
    "print(\"  * Step 4/5: Writing the .stream file...\")\n",
    "write_stream_from_filtered_csv(\n",
    "    filtered_csv_path=FILTERED_CSV_PATH,\n",
    "    output_stream_path=OUTPUT_STREAM_PATH,\n",
    "    event_col=\"event_number\",\n",
    "    streamfile_col=\"stream_file\"\n",
    ")\n",
    "time.sleep(0.2)\n",
    "\n",
    "print(\"  * Step 5/5: Conversion complete!\")\n",
    "print(f\"CSV has been successfully converted to:\\n  {OUTPUT_STREAM_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Merging, SHELX Conversion, and MTZ Conversion\n",
    "\n",
    "This section provides interactive tools to:\n",
    "1. **Merge Results:** Select a .stream file and set parameters (pointgroup, number of threads, iterations) to merge the best indexing results.\n",
    "2. **Convert to SHELX Format:** Convert the merged results into a Shelx-compatible .hkl format.\n",
    "3. **Convert to MTZ Format:** Using a chosen cell file, convert the .hkl file to .mtz format for downstream analysis.\n",
    "\n",
    "Adjust the parameters using the provided widgets, then follow the step-by-step process to execute merging and format conversions.\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Merging Section -----\n",
    "import time\n",
    "from merge_and_convert.merge import merge\n",
    "\n",
    "# Define the parameters for merging.\n",
    "stream_file = \"path/to/your/file.stream\"  # Path to the .stream file\n",
    "pointgroup = \"P212121\"  # Adjust pointgroup as needed\n",
    "num_threads = 24\n",
    "iterations = 5\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MERGING SECTION\")\n",
    "print(\"=\"*50)\n",
    "print(\"Merging in progress...\")\n",
    "time.sleep(0.2)  # Simulate progress\n",
    "\n",
    "# Call the merge function\n",
    "output_dir = merge(\n",
    "    stream_file,\n",
    "    pointgroup=pointgroup,\n",
    "    num_threads=num_threads,\n",
    "    iterations=iterations,\n",
    ")\n",
    "time.sleep(0.2)\n",
    "\n",
    "if output_dir is not None:\n",
    "    print(\"Merging done. Results are in:\", output_dir)\n",
    "else:\n",
    "    print(\"Merging failed. Please check the parameters and try again.\")\n",
    "print(\"Done merging.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- SHELX Conversion Section -----\n",
    "from merge_and_convert.convert_hkl_crystfel_to_shelx import convert_hkl_crystfel_to_shelx \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SHELX CONVERSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if output_dir is None:\n",
    "    print(\"No merged output available. Please run the merge step first.\")\n",
    "else:\n",
    "    print(\"Converting to SHELX...\")\n",
    "    convert_hkl_crystfel_to_shelx(output_dir)\n",
    "    print(\"Conversion to SHELX completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- MTZ Conversion Section -----\n",
    "import os\n",
    "from merge_and_convert.convert_hkl_to_mtz import convert_hkl_to_mtz\n",
    "# Define the cell file path for MTZ conversion.\n",
    "cell_file = \"path/to/your/cell_file.cell\"  # Adjust as needed\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MTZ CONVERSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if output_dir is None:\n",
    "    print(\"No merged output available. Please run the merge step first.\")\n",
    "else:\n",
    "    if not os.path.exists(cell_file):\n",
    "        print(\"Cell file not found. Please check the path:\", cell_file)\n",
    "    else:\n",
    "        print(\"Converting to MTZ...\")\n",
    "        convert_hkl_to_mtz(output_dir, cellfile_path=cell_file)\n",
    "        print(\"Conversion to MTZ completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyxem-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
